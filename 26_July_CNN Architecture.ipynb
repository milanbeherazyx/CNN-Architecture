{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Purpose and Benefits of Pooling in CNN\n",
    "**Purpose**:\n",
    "Pooling is a downsampling technique used in CNNs to reduce the spatial dimensions (width and height) of feature maps, which helps decrease the computational load and reduce the number of parameters in the model.\n",
    "\n",
    "**Benefits**:\n",
    "- **Dimensionality Reduction**: By reducing the spatial size of the feature maps, pooling decreases the computational burden on subsequent layers, allowing for faster processing and reduced memory usage.\n",
    "- **Translation Invariance**: Pooling helps the network become invariant to small translations or distortions in the input image. This means that small changes in the position of features in the input will not significantly affect the output.\n",
    "- **Feature Extraction**: It retains important features while discarding less important ones, helping the network focus on the most relevant patterns in the data.\n",
    "\n",
    "### 2. Difference Between Min Pooling and Max Pooling\n",
    "- **Max Pooling**: This technique selects the maximum value from a specified region (or pooling window) in the feature map. For example, if the pooling window is 2x2, max pooling will take the largest value from each 2x2 region and create a new feature map from these maximum values.\n",
    "\n",
    "- **Min Pooling**: In contrast, min pooling selects the minimum value from the specified region in the feature map. Similar to max pooling, if the pooling window is 2x2, min pooling will take the smallest value from each 2x2 region.\n",
    "\n",
    "**Comparison**:\n",
    "- **Max Pooling** is more commonly used in CNN architectures, as it retains stronger features that are more likely to be relevant for classification.\n",
    "- **Min Pooling** is less frequently used, as it can potentially discard useful information by focusing on weaker features.\n",
    "\n",
    "### 3. Concept of Padding in CNN and Its Significance\n",
    "**Padding** refers to the practice of adding extra pixels (usually zeros) around the border of an input feature map before applying a convolution operation. \n",
    "\n",
    "**Significance**:\n",
    "- **Control Output Size**: Padding allows for control over the output size of the feature maps after convolution. Without padding, the spatial dimensions of the output feature map decrease after each convolutional layer, which can lead to very small feature maps in deeper layers.\n",
    "- **Preserve Spatial Information**: Padding helps preserve the spatial dimensions and important features near the edges of the input images, ensuring that they are not lost during the convolution process.\n",
    "- **Improve Model Performance**: By preserving the size of feature maps and important features, padding can improve the overall performance of the CNN, leading to better feature extraction.\n",
    "\n",
    "### 4. Comparison of Zero-Padding and Valid-Padding\n",
    "- **Zero-Padding**: This method adds zeros to the borders of the input feature map. The amount of padding can be controlled, allowing for output feature maps that maintain the original size (or a specific desired size). For example, with a kernel size of 3x3 and stride of 1, adding one pixel of zero-padding on all sides will maintain the size of the feature map.\n",
    "\n",
    "- **Valid-Padding**: This method does not add any padding to the input feature map. As a result, the spatial dimensions of the output feature map are reduced after each convolution operation. For instance, with a kernel size of 3x3 and stride of 1, the output feature map will be smaller than the input feature map, specifically by 2 pixels in each dimension.\n",
    "\n",
    "**Comparison in Effects**:\n",
    "- **Zero-Padding**: Helps maintain the output size and allows the model to capture features from the edges of the input image.\n",
    "- **Valid-Padding**: Results in smaller output feature maps and may lose information from the edges, but can also reduce the computational burden if the feature map sizes are significantly reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overview of LeNet-5 Architecture\n",
    "LeNet-5, developed by Yann LeCun et al. in the late 1980s, is one of the pioneering architectures for convolutional neural networks (CNNs). It is primarily designed for handwritten digit recognition, particularly on the MNIST dataset. The architecture consists of seven layers (excluding the input layer), organized into two convolutional layers followed by two fully connected layers, with average pooling layers in between.\n",
    "\n",
    "**Architecture Summary**:\n",
    "1. **Input Layer**: Accepts 32x32 grayscale images.\n",
    "2. **Convolutional Layer 1 (C1)**: Applies 6 filters of size 5x5, producing a 28x28 feature map.\n",
    "3. **Activation Layer (S1)**: Uses the sigmoid activation function (historically; modern implementations often use ReLU).\n",
    "4. **Subsampling Layer 1 (S2)**: Average pooling with a 2x2 filter and stride 2, resulting in a 14x14 feature map.\n",
    "5. **Convolutional Layer 2 (C3)**: Applies 16 filters of size 5x5, resulting in a 10x10 feature map.\n",
    "6. **Activation Layer (S3)**: Sigmoid activation function.\n",
    "7. **Subsampling Layer 2 (S4)**: Average pooling with a 2x2 filter and stride 2, resulting in a 5x5 feature map.\n",
    "8. **Fully Connected Layer 1 (C5)**: Flattening the feature map and connecting to 120 neurons.\n",
    "9. **Fully Connected Layer 2 (F6)**: 84 neurons.\n",
    "10. **Output Layer**: 10 neurons for digit classification (0-9).\n",
    "\n",
    "### 2. Key Components of LeNet-5 and Their Purposes\n",
    "- **Convolutional Layers (C1 and C3)**: Extract local features from the input images using learnable filters. They capture spatial hierarchies and patterns like edges, textures, and shapes.\n",
    "\n",
    "- **Activation Functions (S1 and S3)**: Apply nonlinear transformations to the feature maps, allowing the network to learn complex patterns. While LeNet-5 initially used sigmoid activation, modern implementations often use ReLU.\n",
    "\n",
    "- **Subsampling Layers (S2 and S4)**: Perform average pooling to reduce the spatial dimensions of the feature maps while retaining important information. This helps decrease computation and control overfitting.\n",
    "\n",
    "- **Fully Connected Layers (C5 and F6)**: Serve to learn global patterns and relationships among the features extracted by the convolutional layers. The last fully connected layer produces the final output.\n",
    "\n",
    "### 3. Advantages and Limitations of LeNet-5\n",
    "**Advantages**:\n",
    "- **Simplicity**: The architecture is relatively simple and easy to understand, making it an excellent starting point for learning about CNNs.\n",
    "- **Effective for Handwritten Digits**: LeNet-5 is well-suited for recognizing handwritten digits, especially in the MNIST dataset.\n",
    "\n",
    "**Limitations**:\n",
    "- **Shallow Architecture**: LeNet-5 is relatively shallow compared to modern CNNs, which limits its capacity to learn complex features. It may not perform well on more complex datasets or tasks.\n",
    "- **Fixed Input Size**: The model requires fixed-size input images (32x32), which may not be suitable for varying input sizes without preprocessing.\n",
    "- **Obsolete in Modern Applications**: More advanced architectures (e.g., AlexNet, VGG, ResNet) have been developed that outperform LeNet-5 in image classification tasks.\n",
    "\n",
    "### 4. Implementing LeNet-5 Using PyTorch on MNIST\n",
    "Here's an implementation of LeNet-5 in PyTorch and training it on the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.4.0\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: c:\\ProgramData\\anaconda3\\Lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define LeNet-5 architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Instantiate model, define loss function and optimizer\n",
    "model = LeNet5()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy of the model on the test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Insights\n",
    "- **Training**: The model is trained for 10 epochs using the Adam optimizer. The loss decreases over epochs, indicating the model is learning.\n",
    "\n",
    "- **Accuracy**: The accuracy on the MNIST test set is typically high (over 98% is common), showcasing LeNet-5’s effectiveness for simple image classification tasks.\n",
    "\n",
    "### Conclusion\n",
    "LeNet-5 is a foundational CNN architecture that laid the groundwork for more advanced deep learning models. While it has its limitations, it remains a valuable educational tool for understanding CNNs and their components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing AlexNet\n",
    "\n",
    "#### 1. Overview of the AlexNet Architecture\n",
    "AlexNet is a pioneering convolutional neural network (CNN) architecture designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Introduced in the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC), it achieved a significant improvement in image classification tasks, winning the competition by a large margin. The architecture consists of the following components:\n",
    "\n",
    "- **Input Layer**: Accepts images of size 224x224 pixels with 3 color channels (RGB).\n",
    "- **Convolutional Layers**: A series of convolutional layers that extract features from the input images.\n",
    "- **Pooling Layers**: Used to downsample feature maps and reduce dimensionality.\n",
    "- **Fully Connected Layers**: Classify the extracted features into various classes.\n",
    "- **Output Layer**: Produces the final classification scores.\n",
    "\n",
    "The original AlexNet architecture consists of 5 convolutional layers, followed by 3 fully connected layers.\n",
    "\n",
    "#### 2. Architectural Innovations in AlexNet\n",
    "AlexNet introduced several key innovations that contributed to its success:\n",
    "\n",
    "- **ReLU Activation Function**: ReLU (Rectified Linear Unit) replaced traditional activation functions like sigmoid or tanh, allowing for faster convergence during training and reducing the vanishing gradient problem.\n",
    "\n",
    "- **Overlapping Pooling**: Instead of using non-overlapping pooling, AlexNet used overlapping max pooling, which helps retain more spatial information.\n",
    "\n",
    "- **Data Augmentation**: AlexNet employed data augmentation techniques like random cropping, flipping, and color perturbation to increase the diversity of the training dataset, improving generalization.\n",
    "\n",
    "- **Dropout**: To prevent overfitting, the model used dropout layers in the fully connected layers, randomly setting a fraction of the units to zero during training.\n",
    "\n",
    "- **GPU Utilization**: AlexNet was one of the first deep learning models to leverage GPU for training, significantly speeding up the training process.\n",
    "\n",
    "#### 3. Role of Convolutional Layers, Pooling Layers, and Fully Connected Layers in AlexNet\n",
    "- **Convolutional Layers**: These layers apply convolutional filters to the input images, capturing spatial hierarchies and patterns in the data. In AlexNet, the first layer uses large filters to capture broad features, while subsequent layers use smaller filters for finer details.\n",
    "\n",
    "- **Pooling Layers**: Pooling layers reduce the spatial dimensions of the feature maps, which decreases the number of parameters and computation in the network, and helps to control overfitting. AlexNet employs max pooling, which retains the most significant features while discarding less relevant information.\n",
    "\n",
    "- **Fully Connected Layers**: The fully connected layers take the high-level features extracted by the convolutional layers and perform the classification task. In AlexNet, the last fully connected layer outputs a vector of probabilities corresponding to the classes in the dataset.\n",
    "\n",
    "#### 4. Implementing AlexNet\n",
    "Here’s a basic implementation of AlexNet using PyTorch and its evaluation on the CIFAR-10 dataset.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the AlexNet architecture\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.relu(self.conv5(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 256 * 6 * 6)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Instantiate model, define loss function and optimizer\n",
    "model = AlexNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy of the model on the test set: {accuracy:.2f}%\")\n",
    "```\n",
    "\n",
    "### Explanation of the Implementation\n",
    "1. **Model Definition**: The `AlexNet` class defines the architecture, including convolutional layers, pooling layers, fully connected layers, ReLU activation, and dropout for regularization.\n",
    "2. **Data Loading**: The CIFAR-10 dataset is loaded and transformed to fit the input size required by AlexNet.\n",
    "3. **Training Loop**: The model is trained using Adam optimizer and cross-entropy loss, with the loss being printed for each epoch.\n",
    "4. **Evaluation**: The model is evaluated on the test dataset, calculating and printing the accuracy.\n",
    "\n",
    "### Conclusion\n",
    "AlexNet's architecture and innovations significantly impacted the field of deep learning, particularly in image classification. Implementing it and evaluating its performance on datasets like CIFAR-10 helps illustrate its effectiveness and underlying concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
